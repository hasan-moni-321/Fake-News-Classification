{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_news_classification_HPT_95%.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3gxktn3SCTy"
      },
      "source": [
        "### Accessing google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKp269U9jyJ5",
        "outputId": "2a3b0a4a-8807-4731-c4a8-947bd72c3023",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2cd18853-2140-4ee6-8544-c55ada2bdf8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2cd18853-2140-4ee6-8544-c55ada2bdf8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"hasanmoni\",\"key\":\"5a2a53290ee4866258b0dfd1198cabb0\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL9a4ChOj0MJ",
        "outputId": "00415bf1-e51d-48fc-e7d9-3deb92da7729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iif993goSX14"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk6sG66xSYVF"
      },
      "source": [
        "### Loading necessary library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr6290Xuj0Vp",
        "outputId": "b45df4b3-52a7-46b1-988d-b5cc39e4fd53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import random\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import optuna \n",
        "from functools import partial\n",
        "from skopt import space\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmTmDuG0j0Y5"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH1o0zeTSdbV"
      },
      "source": [
        "### Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29tQW6m0j0T1"
      },
      "source": [
        "# Loading dataset\n",
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK6iVkWIj0Pd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjYNNocokcTB"
      },
      "source": [
        "#####################################################################\n",
        "# Feature Engineering\n",
        "######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2qzvzGkkeJY",
        "outputId": "e3243ff0-ab62-483a-b56f-c2e9d91ede49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Checking null data\n",
        "print(data.isnull().sum())\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id           0\n",
            "title      558\n",
            "author    1957\n",
            "text        39\n",
            "label        0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IiVp8PgkePW",
        "outputId": "767b299b-a9a3-48ff-9645-0301b5cf85d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "#total number and percent of null value \n",
        "print('null value number = {} and percent = {} '.format((data['title'].isnull().sum()),   (data['title'].isnull().sum() *100)/len(data['title'])))\n",
        "print('null value number = {} and percent = {} '.format((data['author'].isnull().sum()),   (data['author'].isnull().sum() *100)/len(data['title'])))\n",
        "print('null value number = {} and percent = {} '.format((data['text'].isnull().sum()),   (data['text'].isnull().sum() *100)/len(data['title'])))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "null value number = 558 and percent = 2.6826923076923075 \n",
            "null value number = 1957 and percent = 9.408653846153847 \n",
            "null value number = 39 and percent = 0.1875 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXrmqktykeaZ",
        "outputId": "bdd9dec9-262f-40c7-bfed-67a3760db6ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#dropping null value\n",
        "data.dropna(axis=0, inplace=True)\n",
        "print('Shape after dropping null value :', data.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape after dropping null value : (18285, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QyS-A_rkegK",
        "outputId": "1e85553d-9ad4-488b-997a-2f7c6d88c052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "#checking balanced or imbalanced data\n",
        "sns.countplot(data['label'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning:\n",
            "\n",
            "Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa71af00470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSS59qK8keYX",
        "outputId": "a34d1c24-fc32-4ccc-e7d3-bde5fc99a830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#index resetting\n",
        "data = data.reset_index()\n",
        "data.drop(['index','id'], axis=1, inplace=True)\n",
        "data.columns.rename={'level_0':'id'}\n",
        "print(data.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               title  ... label\n",
            "0  House Dem Aide: We Didn’t Even See Comey’s Let...  ...     1\n",
            "1  FLYNN: Hillary Clinton, Big Woman on Campus - ...  ...     0\n",
            "2                  Why the Truth Might Get You Fired  ...     1\n",
            "3  15 Civilians Killed In Single US Airstrike Hav...  ...     1\n",
            "4  Iranian woman jailed for fictional unpublished...  ...     1\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEwzAkJUkeWE",
        "outputId": "1617a7cc-c135-4420-cad1-0353ce149b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#printing a text\n",
        "print(data['text'][0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It By Darrell Lucus on October 30, 2016 Subscribe Jason Chaffetz on the stump in American Fork, Utah ( image courtesy Michael Jolley, available under a Creative Commons-BY license) \n",
            "With apologies to Keith Olbermann, there is no doubt who the Worst Person in The World is this week–FBI Director James Comey. But according to a House Democratic aide, it looks like we also know who the second-worst person is as well. It turns out that when Comey sent his now-infamous letter announcing that the FBI was looking into emails that may be related to Hillary Clinton’s email server, the ranking Democrats on the relevant committees didn’t hear about it from Comey. They found out via a tweet from one of the Republican committee chairmen. \n",
            "As we now know, Comey notified the Republican chairmen and Democratic ranking members of the House Intelligence, Judiciary, and Oversight committees that his agency was reviewing emails it had recently discovered in order to see if they contained classified information. Not long after this letter went out, Oversight Committee Chairman Jason Chaffetz set the political world ablaze with this tweet. FBI Dir just informed me, \"The FBI has learned of the existence of emails that appear to be pertinent to the investigation.\" Case reopened \n",
            "— Jason Chaffetz (@jasoninthehouse) October 28, 2016 \n",
            "Of course, we now know that this was not the case . Comey was actually saying that it was reviewing the emails in light of “an unrelated case”–which we now know to be Anthony Weiner’s sexting with a teenager. But apparently such little things as facts didn’t matter to Chaffetz. The Utah Republican had already vowed to initiate a raft of investigations if Hillary wins–at least two years’ worth, and possibly an entire term’s worth of them. Apparently Chaffetz thought the FBI was already doing his work for him–resulting in a tweet that briefly roiled the nation before cooler heads realized it was a dud. \n",
            "But according to a senior House Democratic aide, misreading that letter may have been the least of Chaffetz’ sins. That aide told Shareblue that his boss and other Democrats didn’t even know about Comey’s letter at the time–and only found out when they checked Twitter. “Democratic Ranking Members on the relevant committees didn’t receive Comey’s letter until after the Republican Chairmen. In fact, the Democratic Ranking Members didn’ receive it until after the Chairman of the Oversight and Government Reform Committee, Jason Chaffetz, tweeted it out and made it public.” \n",
            "So let’s see if we’ve got this right. The FBI director tells Chaffetz and other GOP committee chairmen about a major development in a potentially politically explosive investigation, and neither Chaffetz nor his other colleagues had the courtesy to let their Democratic counterparts know about it. Instead, according to this aide, he made them find out about it on Twitter. \n",
            "There has already been talk on Daily Kos that Comey himself provided advance notice of this letter to Chaffetz and other Republicans, giving them time to turn on the spin machine. That may make for good theater, but there is nothing so far that even suggests this is the case. After all, there is nothing so far that suggests that Comey was anything other than grossly incompetent and tone-deaf. \n",
            "What it does suggest, however, is that Chaffetz is acting in a way that makes Dan Burton and Darrell Issa look like models of responsibility and bipartisanship. He didn’t even have the decency to notify ranking member Elijah Cummings about something this explosive. If that doesn’t trample on basic standards of fairness, I don’t know what does. \n",
            "Granted, it’s not likely that Chaffetz will have to answer for this. He sits in a ridiculously Republican district anchored in Provo and Orem; it has a Cook Partisan Voting Index of R+25, and gave Mitt Romney a punishing 78 percent of the vote in 2012. Moreover, the Republican House leadership has given its full support to Chaffetz’ planned fishing expedition. But that doesn’t mean we can’t turn the hot lights on him. After all, he is a textbook example of what the House has become under Republican control. And he is also the Second Worst Person in the World. About Darrell Lucus \n",
            "Darrell is a 30-something graduate of the University of North Carolina who considers himself a journalist of the old school. An attempt to turn him into a member of the religious right in college only succeeded in turning him into the religious right's worst nightmare--a charismatic Christian who is an unapologetic liberal. His desire to stand up for those who have been scared into silence only increased when he survived an abusive three-year marriage. You may know him on Daily Kos as Christian Dem in NC . Follow him on Twitter @DarrellLucus or connect with him on Facebook . Click here to buy Darrell a Mello Yello. Connect\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEpclcwikeT1",
        "outputId": "226cc9ef-6e4f-428d-b27e-b576ceda0db0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#printing a text\n",
        "print(data['text'][0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It By Darrell Lucus on October 30, 2016 Subscribe Jason Chaffetz on the stump in American Fork, Utah ( image courtesy Michael Jolley, available under a Creative Commons-BY license) \n",
            "With apologies to Keith Olbermann, there is no doubt who the Worst Person in The World is this week–FBI Director James Comey. But according to a House Democratic aide, it looks like we also know who the second-worst person is as well. It turns out that when Comey sent his now-infamous letter announcing that the FBI was looking into emails that may be related to Hillary Clinton’s email server, the ranking Democrats on the relevant committees didn’t hear about it from Comey. They found out via a tweet from one of the Republican committee chairmen. \n",
            "As we now know, Comey notified the Republican chairmen and Democratic ranking members of the House Intelligence, Judiciary, and Oversight committees that his agency was reviewing emails it had recently discovered in order to see if they contained classified information. Not long after this letter went out, Oversight Committee Chairman Jason Chaffetz set the political world ablaze with this tweet. FBI Dir just informed me, \"The FBI has learned of the existence of emails that appear to be pertinent to the investigation.\" Case reopened \n",
            "— Jason Chaffetz (@jasoninthehouse) October 28, 2016 \n",
            "Of course, we now know that this was not the case . Comey was actually saying that it was reviewing the emails in light of “an unrelated case”–which we now know to be Anthony Weiner’s sexting with a teenager. But apparently such little things as facts didn’t matter to Chaffetz. The Utah Republican had already vowed to initiate a raft of investigations if Hillary wins–at least two years’ worth, and possibly an entire term’s worth of them. Apparently Chaffetz thought the FBI was already doing his work for him–resulting in a tweet that briefly roiled the nation before cooler heads realized it was a dud. \n",
            "But according to a senior House Democratic aide, misreading that letter may have been the least of Chaffetz’ sins. That aide told Shareblue that his boss and other Democrats didn’t even know about Comey’s letter at the time–and only found out when they checked Twitter. “Democratic Ranking Members on the relevant committees didn’t receive Comey’s letter until after the Republican Chairmen. In fact, the Democratic Ranking Members didn’ receive it until after the Chairman of the Oversight and Government Reform Committee, Jason Chaffetz, tweeted it out and made it public.” \n",
            "So let’s see if we’ve got this right. The FBI director tells Chaffetz and other GOP committee chairmen about a major development in a potentially politically explosive investigation, and neither Chaffetz nor his other colleagues had the courtesy to let their Democratic counterparts know about it. Instead, according to this aide, he made them find out about it on Twitter. \n",
            "There has already been talk on Daily Kos that Comey himself provided advance notice of this letter to Chaffetz and other Republicans, giving them time to turn on the spin machine. That may make for good theater, but there is nothing so far that even suggests this is the case. After all, there is nothing so far that suggests that Comey was anything other than grossly incompetent and tone-deaf. \n",
            "What it does suggest, however, is that Chaffetz is acting in a way that makes Dan Burton and Darrell Issa look like models of responsibility and bipartisanship. He didn’t even have the decency to notify ranking member Elijah Cummings about something this explosive. If that doesn’t trample on basic standards of fairness, I don’t know what does. \n",
            "Granted, it’s not likely that Chaffetz will have to answer for this. He sits in a ridiculously Republican district anchored in Provo and Orem; it has a Cook Partisan Voting Index of R+25, and gave Mitt Romney a punishing 78 percent of the vote in 2012. Moreover, the Republican House leadership has given its full support to Chaffetz’ planned fishing expedition. But that doesn’t mean we can’t turn the hot lights on him. After all, he is a textbook example of what the House has become under Republican control. And he is also the Second Worst Person in the World. About Darrell Lucus \n",
            "Darrell is a 30-something graduate of the University of North Carolina who considers himself a journalist of the old school. An attempt to turn him into a member of the religious right in college only succeeded in turning him into the religious right's worst nightmare--a charismatic Christian who is an unapologetic liberal. His desire to stand up for those who have been scared into silence only increased when he survived an abusive three-year marriage. You may know him on Daily Kos as Christian Dem in NC . Follow him on Twitter @DarrellLucus or connect with him on Facebook . Click here to buy Darrell a Mello Yello. Connect\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_qZuKOXkqlq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYhAVUrFkxPc"
      },
      "source": [
        "###############################################################################\n",
        "# Text cleaning\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSEgkXOFkqpy"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for row in range(0,len(data)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', data['text'][row])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzI7zz6ckq5j",
        "outputId": "83f9cf5d-23d3-4dcf-ddcc-42a80a175553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#printing a paragraph\n",
        "print(corpus[2])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truth might get fire octob tension intellig analyst polit policymak alway honest assess desir result latter often overwhelm former iraq war write lawrenc davidson lawrenc davidson might wonder foreign polici maker repeatedli make bad choic insight might drawn follow analysi action play unit state lesson probabl univers back earli spring georg w bush initi invas iraq one key public reason claim countri dictat saddam hussein verg develop nuclear weapon hide weapon mass destruct real reason went beyond charg includ long rang plan regim chang middl east presid georg w bush vice presid dick cheney receiv oval offic brief cia director georg tenet also present chief staff andi card right white hous photo purpos concentr belief iraq becom hostil nuclear power presid bush close associ accept scenario readili short answer bush want inde need believ rational invad iraq first tri connect saddam hussein attack u though never gave stratagem lack evid made difficult ralli american peopl alreadi fixat afghanistan support war baghdad nuclear weapon gambit prove fruit hard evid charg supposedli reliabl wit person exil anti saddam iraqi mani u govern payrol kept tell bush advis nuclear stori true u leadership cadr whose worldview liter demand mortal danger iraq inform order precipit overthrow saddam will tell tale pend atom weapon strong desir believ tale nuclear iraq lower threshold proof likewis repeat assert assum depend iraqi sourc underpin nationwid u campaign gener fear war fever u alli insist unit nation send weapon inspector scour iraq evid nuclear weapon program well chemic biolog weapon inspector could find convinc evid frustrat bush administr soon forc hand march bush launch invas iraq expect occup countri u inspector would sure find evid nuke least stockpil chemic biolog weapon iraqi inform systemat lie social behavior scienc rescu variou u intellig agenc thoroughli shaken affair today year later director manag still tri sort specif tell get true intellig lie one intellig worker put need help protect us armi snake oil salesmen end cia et al market academ assist ahm chalabi head iraqi nation congress key supplier iraqi defector bogu stori hidden wmd partnership forg offic director nation intellig odni serv coordin center sixteen independ u intellig agenc nation academi scienc engin medicin result collabor perman intellig commun studi board coordin program social behavior scienc research might strengthen nation secur despit effort almost certain social behavior scienc cannot give spi agenc want way detect lie better present standard procedur polygraph test interrog even could might well make differ real problem found liar found believ believ simpli true odni leader seem assert u intellig agenc personnel cannot tell often lie case thousand middl echelon intellig worker desk offic specialist know someth close approach truth know pretti well go place like afghanistan iraq syria libya israel palestin elsewher director nation intellig jame clapper right talk presid barack obama oval offic john brennan nation secur aid present photo credit offic director nation intellig therefor someon feed snake oil usual know howev accur grasp thing often avail superior got appoint accept pre structur worldview differ criterion true analyst listen charl gaukel nation intellig council yet anoth organ act meet ground intellig agenc refer search way avoid get taken lie gaukel declar look truth particularli look truth work might mean certainli tell mean histor mean power broker truth must match fit worldview polit ideolog precept fit work intellig specialist send usual accur assess line polici maker often hit roadblock caus group think ideolog blinker know better attitud hand long sell leadership match want believ peddl anyth imaginari iraqi nuke israel western style democraci saudi arabia indispens alli libya liber countri bashar al assad real roadblock peac syria strateg defens initi sdi aka star war world get colder warmer american exception glori list almost endless sad tale tell us want spend million dollar social behavior scienc research improv assess use intellig forget liar want look antidot narrow minded believ policymak seem abl rise ideolog presumpt class presumpt underpin self confid lead us slipperi slope happen way often mani place sourc shakespear determin past prelud elit play destini free capac break structur way see yet middl echelon specialist keep send rel accur assess ladder power hope spring etern\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC5OMjPpkq3K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7PSqi3dk6bh"
      },
      "source": [
        "##############################################################################\n",
        "# Encoding\n",
        "##############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqsmbfcjkq06",
        "outputId": "761f1361-40c2-4d03-b9c6-b096dc00e435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Bag of Words using TfidfVectorizer()\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n",
        "X = tfidf.fit_transform(corpus).toarray()\n",
        "print('Shape of X is :',X.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X is : (18285, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrcJc_UpkqyC",
        "outputId": "da9d4616-db8d-4763-f818-c5723408c1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#printing some feature names\n",
        "print(tfidf.get_feature_names()[:10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['aaron', 'abandon', 'abc', 'abe', 'abedin', 'abil', 'abl', 'abort', 'abroad', 'absenc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fe2C80GkqwE",
        "outputId": "0839f350-232f-4ca1-d3b4-4e9532a3275d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#printing parameter\n",
        "print(tfidf.get_params())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': 5000, 'min_df': 1, 'ngram_range': (1, 3), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQE9oGpOkquM",
        "outputId": "99e83202-271e-42ee-9628-1eddfc8960e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#label data\n",
        "y = data['label']\n",
        "print('Length of y :',len(y))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of y : 18285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcRRzja7lF2y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttulEziUlF95"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWA_omehlIyy"
      },
      "source": [
        "##########################################################################\n",
        "# Dividing dataset\n",
        "##########################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSk_JRWslGAu",
        "outputId": "1da742c0-2ae4-49ab-aa13-6e2e63e5ed94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "#dividing dataset for train and test\n",
        "Xtrain,xtest,Ytrain,ytest = train_test_split(X,y, test_size=.2, random_state=0)\n",
        "print('Shape of Xtrain :',Xtrain.shape)\n",
        "print('Shape of xtest :',xtest.shape)\n",
        "print('Shap of Ytrain :',Ytrain.shape)\n",
        "print('Shape of ytest :',ytest.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Xtrain : (14628, 5000)\n",
            "Shape of xtest : (3657, 5000)\n",
            "Shap of Ytrain : (14628,)\n",
            "Shape of ytest : (3657,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljfx3uHWlF7K",
        "outputId": "c8b5543a-8aac-488a-9d97-e03b6fd3a82f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "#printing some BOW words data\n",
        "df = pd.DataFrame(Xtrain, columns=tfidf.get_feature_names())\n",
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aaron</th>\n",
              "      <th>abandon</th>\n",
              "      <th>abc</th>\n",
              "      <th>abe</th>\n",
              "      <th>abedin</th>\n",
              "      <th>abil</th>\n",
              "      <th>abl</th>\n",
              "      <th>abort</th>\n",
              "      <th>abroad</th>\n",
              "      <th>absenc</th>\n",
              "      <th>absolut</th>\n",
              "      <th>absorb</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abu</th>\n",
              "      <th>abus</th>\n",
              "      <th>academ</th>\n",
              "      <th>academi</th>\n",
              "      <th>acceler</th>\n",
              "      <th>accept</th>\n",
              "      <th>access</th>\n",
              "      <th>access pipelin</th>\n",
              "      <th>accid</th>\n",
              "      <th>accommod</th>\n",
              "      <th>accompani</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>accord</th>\n",
              "      <th>accord report</th>\n",
              "      <th>account</th>\n",
              "      <th>accumul</th>\n",
              "      <th>accur</th>\n",
              "      <th>accus</th>\n",
              "      <th>achiev</th>\n",
              "      <th>acid</th>\n",
              "      <th>acknowledg</th>\n",
              "      <th>acquir</th>\n",
              "      <th>acr</th>\n",
              "      <th>across</th>\n",
              "      <th>across countri</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>...</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrongdo</th>\n",
              "      <th>wrote</th>\n",
              "      <th>wrote twitter</th>\n",
              "      <th>www</th>\n",
              "      <th>xi</th>\n",
              "      <th>yahoo</th>\n",
              "      <th>yard</th>\n",
              "      <th>ye</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>year ago</th>\n",
              "      <th>year later</th>\n",
              "      <th>year mr</th>\n",
              "      <th>year old</th>\n",
              "      <th>year said</th>\n",
              "      <th>year sinc</th>\n",
              "      <th>yell</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yemen</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>yet anoth</th>\n",
              "      <th>yiannopoulo</th>\n",
              "      <th>yield</th>\n",
              "      <th>york</th>\n",
              "      <th>york citi</th>\n",
              "      <th>york time</th>\n",
              "      <th>yorker</th>\n",
              "      <th>young</th>\n",
              "      <th>young peopl</th>\n",
              "      <th>younger</th>\n",
              "      <th>youth</th>\n",
              "      <th>youtub</th>\n",
              "      <th>zero</th>\n",
              "      <th>zika</th>\n",
              "      <th>zionist</th>\n",
              "      <th>zone</th>\n",
              "      <th>zu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.068937</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.119721</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076599</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.175088</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016152</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011834</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.156408</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028654</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017074</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.04646</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.074315</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.037168</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025591</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.022699</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aaron  abandon  abc  abe  abedin  ...  zero  zika  zionist     zone   zu\n",
              "0    0.0      0.0  0.0  0.0     0.0  ...   0.0   0.0      0.0  0.00000  0.0\n",
              "1    0.0      0.0  0.0  0.0     0.0  ...   0.0   0.0      0.0  0.00000  0.0\n",
              "2    0.0      0.0  0.0  0.0     0.0  ...   0.0   0.0      0.0  0.04646  0.0\n",
              "3    0.0      0.0  0.0  0.0     0.0  ...   0.0   0.0      0.0  0.00000  0.0\n",
              "4    0.0      0.0  0.0  0.0     0.0  ...   0.0   0.0      0.0  0.00000  0.0\n",
              "\n",
              "[5 rows x 5000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyn15ynPlF5-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3W1IX0rlPlr"
      },
      "source": [
        "\n",
        "##########################################################################\n",
        "# Hyperparameter tuning using optuna\n",
        "##########################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUG1dOEFlFzV"
      },
      "source": [
        "def optimize(trial, x, y):\n",
        "    C = trial.suggest_uniform('C', .001, 1)\n",
        "    average = trial.suggest_categorical('average', [True, False])\n",
        "    early_stopping = trial.suggest_categorical('early_stopping', [True, False])\n",
        "    fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
        "    max_iter = trial.suggest_int('max_iter', 100, 1500)\n",
        "    tol = trial.suggest_uniform('tol', 0.001, .08)\n",
        "    validation_fraction = trial.suggest_uniform('validation_fraction', 0.1, 0.9)\n",
        "    n_iter_no_change = trial.suggest_int('n_iter_no_change=5', 1, 9)\n",
        "    shuffle = trial.suggest_categorical('shuffle', [True, False])\n",
        "    verbose = trial.suggest_int('verbose', 0, 3)\n",
        "    n_jobs = trial.suggest_int('n_jobs', 0, 5)\n",
        "    random_state = trial.suggest_int('random_state', 0, 5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
        "    #n_estimators = trial.suggest_int(\"n_estimators\", 100, 1500)\n",
        "    #max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
        "    #max_features = trial.suggest_uniform(\"max_features\", 0.01, 1.0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    model = PassiveAggressiveClassifier(\n",
        "        C = C,\n",
        "        average = average,\n",
        "        early_stopping = early_stopping,\n",
        "        fit_intercept = fit_intercept,\n",
        "        max_iter = max_iter,\n",
        "        tol = tol,\n",
        "        validation_fraction = validation_fraction,\n",
        "        n_iter_no_change = n_iter_no_change,\n",
        "        shuffle = shuffle,\n",
        "        verbose = verbose,\n",
        "        n_jobs = n_jobs,\n",
        "        random_state = random_state,\n",
        "        )\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "    accuracies = []\n",
        "    for idx in kf.split(X=x, y=y):\n",
        "        train_idx, test_idx = idx[0], idx[1]\n",
        "        xtrain = x[train_idx]\n",
        "        ytrain = y[train_idx]\n",
        "        \n",
        "        xtest = x[test_idx]\n",
        "        ytest = y[test_idx]\n",
        "        \n",
        "        model.fit(xtrain, ytrain)\n",
        "        preds = model.predict(xtest)\n",
        "        fold_acc = metrics.accuracy_score(ytest, preds)\n",
        "        accuracies.append(fold_acc)\n",
        "        \n",
        "    return -1.0 * np.mean(accuracies)   # printing accuracy in negative\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErUE82SFlTXX"
      },
      "source": [
        "optimization_function = partial(optimize, x=X, y=y)  "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1E3jBoPlTai",
        "outputId": "f31d0e0b-b3a1-4f98-ca27-34ac3015741d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(optimization_function, n_trials=15)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:37:37,898] A new study created in memory with name: no-name-aa68cf24-a36e-47fc-9ff6-e2c7558a4c66\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 22.64, NNZs: 4987, Bias: 0.000000, T: 2900, Avg. loss: 0.073941\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 28.07, NNZs: 4987, Bias: 0.000000, T: 5800, Avg. loss: 0.030430\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31.63, NNZs: 4987, Bias: 0.000000, T: 8700, Avg. loss: 0.018898\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 34.24, NNZs: 4987, Bias: 0.000000, T: 11600, Avg. loss: 0.012703\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 36.15, NNZs: 4987, Bias: 0.000000, T: 14500, Avg. loss: 0.008799\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 37.48, NNZs: 4987, Bias: 0.000000, T: 17400, Avg. loss: 0.006181\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 38.39, NNZs: 4987, Bias: 0.000000, T: 20300, Avg. loss: 0.004516\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 39.04, NNZs: 4987, Bias: 0.000000, T: 23200, Avg. loss: 0.003491\n",
            "Total training time: 1.36 seconds.\n",
            "Convergence after 8 epochs took 1.46 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22.72, NNZs: 4991, Bias: 0.000000, T: 2902, Avg. loss: 0.071558\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 28.00, NNZs: 4992, Bias: 0.000000, T: 5804, Avg. loss: 0.028588\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31.36, NNZs: 4992, Bias: 0.000000, T: 8706, Avg. loss: 0.017377\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 33.73, NNZs: 4992, Bias: 0.000000, T: 11608, Avg. loss: 0.011676\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 35.53, NNZs: 4992, Bias: 0.000000, T: 14510, Avg. loss: 0.008278\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 36.89, NNZs: 4992, Bias: 0.000000, T: 17412, Avg. loss: 0.006030\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 37.82, NNZs: 4992, Bias: 0.000000, T: 20314, Avg. loss: 0.004317\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 38.51, NNZs: 4992, Bias: 0.000000, T: 23216, Avg. loss: 0.003244\n",
            "Total training time: 1.39 seconds.\n",
            "Convergence after 8 epochs took 1.50 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22.27, NNZs: 4993, Bias: 0.000000, T: 2898, Avg. loss: 0.072717\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 27.71, NNZs: 4994, Bias: 0.000000, T: 5796, Avg. loss: 0.030356\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31.33, NNZs: 4994, Bias: 0.000000, T: 8694, Avg. loss: 0.018927\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 33.91, NNZs: 4994, Bias: 0.000000, T: 11592, Avg. loss: 0.012812\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 35.79, NNZs: 4994, Bias: 0.000000, T: 14490, Avg. loss: 0.008997\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 37.18, NNZs: 4994, Bias: 0.000000, T: 17388, Avg. loss: 0.006550\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 38.21, NNZs: 4994, Bias: 0.000000, T: 20286, Avg. loss: 0.004860\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 38.94, NNZs: 4994, Bias: 0.000000, T: 23184, Avg. loss: 0.003624\n",
            "Total training time: 1.35 seconds.\n",
            "Convergence after 8 epochs took 1.45 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22.44, NNZs: 4995, Bias: 0.000000, T: 2897, Avg. loss: 0.072656\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 27.91, NNZs: 4995, Bias: 0.000000, T: 5794, Avg. loss: 0.029405\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31.42, NNZs: 4995, Bias: 0.000000, T: 8691, Avg. loss: 0.017970\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 33.85, NNZs: 4995, Bias: 0.000000, T: 11588, Avg. loss: 0.012001\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 35.64, NNZs: 4995, Bias: 0.000000, T: 14485, Avg. loss: 0.008479\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 36.95, NNZs: 4995, Bias: 0.000000, T: 17382, Avg. loss: 0.006124\n",
            "Total training time: 1.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 37.88, NNZs: 4995, Bias: 0.000000, T: 20279, Avg. loss: 0.004513\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 38.54, NNZs: 4995, Bias: 0.000000, T: 23176, Avg. loss: 0.003418\n",
            "Total training time: 1.40 seconds.\n",
            "Convergence after 8 epochs took 1.50 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22.25, NNZs: 4992, Bias: 0.000000, T: 2898, Avg. loss: 0.070666\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 27.82, NNZs: 4992, Bias: 0.000000, T: 5796, Avg. loss: 0.028454\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31.25, NNZs: 4992, Bias: 0.000000, T: 8694, Avg. loss: 0.017173\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 33.74, NNZs: 4992, Bias: 0.000000, T: 11592, Avg. loss: 0.011525\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 35.50, NNZs: 4992, Bias: 0.000000, T: 14490, Avg. loss: 0.007918\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 36.72, NNZs: 4993, Bias: 0.000000, T: 17388, Avg. loss: 0.005616\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 37.60, NNZs: 4993, Bias: 0.000000, T: 20286, Avg. loss: 0.004203\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 38.24, NNZs: 4993, Bias: 0.000000, T: 23184, Avg. loss: 0.003229\n",
            "Total training time: 1.35 seconds.\n",
            "Convergence after 8 epochs took 1.45 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:37:47,342] Trial 0 finished with value: -0.9283565764287669 and parameters: {'C': 0.40394532236339953, 'average': True, 'early_stopping': True, 'fit_intercept': False, 'max_iter': 118, 'tol': 0.035707075930749364, 'validation_fraction': 0.8006718555316609, 'n_iter_no_change=5': 7, 'shuffle': False, 'verbose': 1, 'n_jobs': 0, 'random_state': 5}. Best is trial 0 with value: -0.9283565764287669.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 33.42, NNZs: 4996, Bias: 1.014831, T: 14557, Avg. loss: 0.258370\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 41.86, NNZs: 4996, Bias: 1.093092, T: 29114, Avg. loss: 0.129136\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.88, NNZs: 4996, Bias: 0.917228, T: 43671, Avg. loss: 0.098097\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.63, NNZs: 4996, Bias: 0.993149, T: 58228, Avg. loss: 0.080295\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.73, NNZs: 4996, Bias: 0.952404, T: 72785, Avg. loss: 0.068692\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 60.28, NNZs: 4996, Bias: 1.134905, T: 87342, Avg. loss: 0.059031\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 63.47, NNZs: 4996, Bias: 1.313868, T: 101899, Avg. loss: 0.051628\n",
            "Total training time: 1.61 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 66.36, NNZs: 4996, Bias: 1.316440, T: 116456, Avg. loss: 0.045651\n",
            "Total training time: 1.83 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 68.96, NNZs: 4996, Bias: 1.373277, T: 131013, Avg. loss: 0.040302\n",
            "Total training time: 2.05 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 71.30, NNZs: 4996, Bias: 1.439489, T: 145570, Avg. loss: 0.035995\n",
            "Total training time: 2.28 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 73.40, NNZs: 4996, Bias: 1.387850, T: 160127, Avg. loss: 0.032251\n",
            "Total training time: 2.50 seconds.\n",
            "Convergence after 11 epochs took 2.51 seconds\n",
            "-- Epoch 1\n",
            "Norm: 33.35, NNZs: 4999, Bias: 1.021616, T: 14556, Avg. loss: 0.252651\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 41.69, NNZs: 4999, Bias: 1.157622, T: 29112, Avg. loss: 0.124887\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.71, NNZs: 4999, Bias: 0.925084, T: 43668, Avg. loss: 0.094946\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.47, NNZs: 4999, Bias: 0.698211, T: 58224, Avg. loss: 0.076890\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.44, NNZs: 4999, Bias: 0.924004, T: 72780, Avg. loss: 0.064659\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59.78, NNZs: 4999, Bias: 1.057719, T: 87336, Avg. loss: 0.056065\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 62.75, NNZs: 4999, Bias: 1.079473, T: 101892, Avg. loss: 0.049338\n",
            "Total training time: 1.63 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 65.42, NNZs: 4999, Bias: 1.186648, T: 116448, Avg. loss: 0.043679\n",
            "Total training time: 1.86 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 67.85, NNZs: 4999, Bias: 1.238843, T: 131004, Avg. loss: 0.038798\n",
            "Total training time: 2.09 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 70.08, NNZs: 4999, Bias: 1.236329, T: 145560, Avg. loss: 0.034989\n",
            "Total training time: 2.31 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 72.14, NNZs: 4999, Bias: 1.264356, T: 160116, Avg. loss: 0.031746\n",
            "Total training time: 2.53 seconds.\n",
            "Convergence after 11 epochs took 2.53 seconds\n",
            "-- Epoch 1\n",
            "Norm: 33.46, NNZs: 4999, Bias: 0.999366, T: 14555, Avg. loss: 0.259108\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 41.86, NNZs: 4999, Bias: 1.010510, T: 29110, Avg. loss: 0.128520\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.85, NNZs: 4999, Bias: 0.894653, T: 43665, Avg. loss: 0.098225\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.70, NNZs: 4999, Bias: 0.948415, T: 58220, Avg. loss: 0.080998\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.76, NNZs: 4999, Bias: 1.037545, T: 72775, Avg. loss: 0.068817\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 60.25, NNZs: 4999, Bias: 1.066094, T: 87330, Avg. loss: 0.059341\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 63.33, NNZs: 4999, Bias: 1.092912, T: 101885, Avg. loss: 0.052079\n",
            "Total training time: 1.62 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 66.12, NNZs: 4999, Bias: 1.131087, T: 116440, Avg. loss: 0.045889\n",
            "Total training time: 1.84 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 68.66, NNZs: 4999, Bias: 1.182339, T: 130995, Avg. loss: 0.041214\n",
            "Total training time: 2.06 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 70.93, NNZs: 4999, Bias: 1.215283, T: 145550, Avg. loss: 0.036611\n",
            "Total training time: 2.29 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 73.04, NNZs: 4999, Bias: 1.198256, T: 160105, Avg. loss: 0.033278\n",
            "Total training time: 2.51 seconds.\n",
            "Convergence after 11 epochs took 2.51 seconds\n",
            "-- Epoch 1\n",
            "Norm: 33.73, NNZs: 4999, Bias: 0.918725, T: 14551, Avg. loss: 0.253996\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.09, NNZs: 4999, Bias: 1.080133, T: 29102, Avg. loss: 0.124630\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.08, NNZs: 4999, Bias: 0.954634, T: 43653, Avg. loss: 0.094299\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.86, NNZs: 4999, Bias: 0.972585, T: 58204, Avg. loss: 0.077480\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.90, NNZs: 4999, Bias: 0.992570, T: 72755, Avg. loss: 0.065486\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 60.31, NNZs: 4999, Bias: 0.965805, T: 87306, Avg. loss: 0.056478\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 63.34, NNZs: 4999, Bias: 1.124972, T: 101857, Avg. loss: 0.049483\n",
            "Total training time: 1.61 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 65.97, NNZs: 4999, Bias: 1.190952, T: 116408, Avg. loss: 0.043722\n",
            "Total training time: 1.84 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 68.36, NNZs: 4999, Bias: 1.176154, T: 130959, Avg. loss: 0.039073\n",
            "Total training time: 2.07 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 70.57, NNZs: 4999, Bias: 1.168966, T: 145510, Avg. loss: 0.035248\n",
            "Total training time: 2.29 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 72.59, NNZs: 4999, Bias: 1.248483, T: 160061, Avg. loss: 0.031797\n",
            "Total training time: 2.51 seconds.\n",
            "Convergence after 11 epochs took 2.51 seconds\n",
            "-- Epoch 1\n",
            "Norm: 33.22, NNZs: 4999, Bias: 2.246373, T: 14553, Avg. loss: 0.255012\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 41.65, NNZs: 5000, Bias: 2.089284, T: 29106, Avg. loss: 0.128082\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.59, NNZs: 5000, Bias: 2.182889, T: 43659, Avg. loss: 0.097573\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.32, NNZs: 5000, Bias: 2.290513, T: 58212, Avg. loss: 0.080261\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.31, NNZs: 5000, Bias: 2.274845, T: 72765, Avg. loss: 0.068574\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59.74, NNZs: 5000, Bias: 2.398993, T: 87318, Avg. loss: 0.059673\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 62.81, NNZs: 5000, Bias: 2.270397, T: 101871, Avg. loss: 0.052584\n",
            "Total training time: 1.62 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 65.57, NNZs: 5000, Bias: 2.167266, T: 116424, Avg. loss: 0.046692\n",
            "Total training time: 1.85 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 68.06, NNZs: 5000, Bias: 2.159513, T: 130977, Avg. loss: 0.041843\n",
            "Total training time: 2.08 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 70.37, NNZs: 5000, Bias: 2.145351, T: 145530, Avg. loss: 0.037828\n",
            "Total training time: 2.30 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 72.47, NNZs: 5000, Bias: 2.134452, T: 160083, Avg. loss: 0.034362\n",
            "Total training time: 2.53 seconds.\n",
            "Convergence after 11 epochs took 2.53 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:38:01,350] Trial 1 finished with value: -0.9468963631391851 and parameters: {'C': 0.24464824862337942, 'average': False, 'early_stopping': False, 'fit_intercept': True, 'max_iter': 1190, 'tol': 0.02811335843988968, 'validation_fraction': 0.8565651203585133, 'n_iter_no_change=5': 8, 'shuffle': False, 'verbose': 3, 'n_jobs': 3, 'random_state': 0}. Best is trial 1 with value: -0.9468963631391851.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 34.18, NNZs: 4996, Bias: 0.000000, T: 9927, Avg. loss: 0.172177\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.68, NNZs: 4996, Bias: 0.000000, T: 19854, Avg. loss: 0.081736\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.60, NNZs: 4996, Bias: 0.000000, T: 29781, Avg. loss: 0.058447\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 53.20, NNZs: 4996, Bias: 0.000000, T: 39708, Avg. loss: 0.045372\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57.01, NNZs: 4996, Bias: 0.000000, T: 49635, Avg. loss: 0.036752\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 60.20, NNZs: 4996, Bias: 0.000000, T: 59562, Avg. loss: 0.030306\n",
            "Total training time: 1.55 seconds.\n",
            "Convergence after 6 epochs took 1.66 seconds\n",
            "-- Epoch 1\n",
            "Norm: 33.92, NNZs: 4993, Bias: 0.000000, T: 9926, Avg. loss: 0.167958\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.46, NNZs: 4994, Bias: 0.000000, T: 19852, Avg. loss: 0.079935\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.32, NNZs: 4994, Bias: 0.000000, T: 29778, Avg. loss: 0.056531\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.84, NNZs: 4994, Bias: 0.000000, T: 39704, Avg. loss: 0.043652\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.53, NNZs: 4994, Bias: 0.000000, T: 49630, Avg. loss: 0.035282\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59.54, NNZs: 4994, Bias: 0.000000, T: 59556, Avg. loss: 0.029076\n",
            "Total training time: 1.52 seconds.\n",
            "Convergence after 6 epochs took 1.62 seconds\n",
            "-- Epoch 1\n",
            "Norm: 34.16, NNZs: 4994, Bias: 0.000000, T: 9924, Avg. loss: 0.173613\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.60, NNZs: 4994, Bias: 0.000000, T: 19848, Avg. loss: 0.084434\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.64, NNZs: 4994, Bias: 0.000000, T: 29772, Avg. loss: 0.061560\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 53.36, NNZs: 4994, Bias: 0.000000, T: 39696, Avg. loss: 0.048205\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57.29, NNZs: 4994, Bias: 0.000000, T: 49620, Avg. loss: 0.039394\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 60.62, NNZs: 4994, Bias: 0.000000, T: 59544, Avg. loss: 0.032836\n",
            "Total training time: 1.52 seconds.\n",
            "Convergence after 6 epochs took 1.62 seconds\n",
            "-- Epoch 1\n",
            "Norm: 34.44, NNZs: 4995, Bias: 0.000000, T: 9919, Avg. loss: 0.169993\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.66, NNZs: 4995, Bias: 0.000000, T: 19838, Avg. loss: 0.081219\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.50, NNZs: 4995, Bias: 0.000000, T: 29757, Avg. loss: 0.058691\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 53.08, NNZs: 4995, Bias: 0.000000, T: 39676, Avg. loss: 0.045980\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.85, NNZs: 4995, Bias: 0.000000, T: 49595, Avg. loss: 0.037511\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 60.03, NNZs: 4995, Bias: 0.000000, T: 59514, Avg. loss: 0.031284\n",
            "Total training time: 1.54 seconds.\n",
            "Convergence after 6 epochs took 1.64 seconds\n",
            "-- Epoch 1\n",
            "Norm: 34.17, NNZs: 4995, Bias: 0.000000, T: 9920, Avg. loss: 0.169792\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.24, NNZs: 4995, Bias: 0.000000, T: 19840, Avg. loss: 0.081672\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.13, NNZs: 4995, Bias: 0.000000, T: 29760, Avg. loss: 0.059829\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.81, NNZs: 4995, Bias: 0.000000, T: 39680, Avg. loss: 0.047128\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.68, NNZs: 4995, Bias: 0.000000, T: 49600, Avg. loss: 0.038354\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59.90, NNZs: 4995, Bias: 0.000000, T: 59520, Avg. loss: 0.031911\n",
            "Total training time: 1.52 seconds.\n",
            "Convergence after 6 epochs took 1.62 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:38:11,386] Trial 2 finished with value: -0.9412633305988516 and parameters: {'C': 0.42403862120504776, 'average': False, 'early_stopping': True, 'fit_intercept': False, 'max_iter': 1130, 'tol': 0.03500852039842981, 'validation_fraction': 0.3182033873288497, 'n_iter_no_change=5': 5, 'shuffle': False, 'verbose': 3, 'n_jobs': 2, 'random_state': 5}. Best is trial 1 with value: -0.9468963631391851.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 44.20, NNZs: 4998, Bias: 1.108147, T: 14557, Avg. loss: 0.235152\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 55.71, NNZs: 4998, Bias: 0.853782, T: 29114, Avg. loss: 0.112672\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 63.44, NNZs: 4998, Bias: 1.084343, T: 43671, Avg. loss: 0.082688\n",
            "Total training time: 0.71 seconds.\n",
            "Convergence after 3 epochs took 0.71 seconds\n",
            "-- Epoch 1\n",
            "Norm: 44.30, NNZs: 4996, Bias: 1.072340, T: 14556, Avg. loss: 0.233612\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 55.53, NNZs: 4997, Bias: 0.023222, T: 29112, Avg. loss: 0.110263\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 63.14, NNZs: 4997, Bias: 1.072429, T: 43668, Avg. loss: 0.080347\n",
            "Total training time: 0.70 seconds.\n",
            "Convergence after 3 epochs took 0.70 seconds\n",
            "-- Epoch 1\n",
            "Norm: 44.66, NNZs: 5000, Bias: 1.039417, T: 14555, Avg. loss: 0.238465\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 55.99, NNZs: 5000, Bias: 0.280647, T: 29110, Avg. loss: 0.116071\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 63.87, NNZs: 5000, Bias: 1.278807, T: 43665, Avg. loss: 0.085381\n",
            "Total training time: 0.71 seconds.\n",
            "Convergence after 3 epochs took 0.71 seconds\n",
            "-- Epoch 1\n",
            "Norm: 45.03, NNZs: 4998, Bias: 1.172762, T: 14551, Avg. loss: 0.232386\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 55.64, NNZs: 4998, Bias: 0.896430, T: 29102, Avg. loss: 0.111649\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 63.54, NNZs: 4998, Bias: 1.296114, T: 43653, Avg. loss: 0.080546\n",
            "Total training time: 0.70 seconds.\n",
            "Convergence after 3 epochs took 0.70 seconds\n",
            "-- Epoch 1\n",
            "Norm: 44.72, NNZs: 4997, Bias: 1.390268, T: 14553, Avg. loss: 0.232484\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 55.59, NNZs: 4998, Bias: 0.463193, T: 29106, Avg. loss: 0.113714\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 63.38, NNZs: 4998, Bias: 1.137431, T: 43659, Avg. loss: 0.083380\n",
            "Total training time: 0.70 seconds.\n",
            "Convergence after 3 epochs took 0.70 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:38:16,340] Trial 3 finished with value: -0.9561936013125513 and parameters: {'C': 0.5228491974484089, 'average': False, 'early_stopping': False, 'fit_intercept': True, 'max_iter': 1245, 'tol': 0.0684483076130318, 'validation_fraction': 0.2170058500075257, 'n_iter_no_change=5': 1, 'shuffle': True, 'verbose': 2, 'n_jobs': 0, 'random_state': 5}. Best is trial 3 with value: -0.9561936013125513.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 38.21, NNZs: 4997, Bias: 0.000000, T: 12439, Avg. loss: 0.196604\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 47.53, NNZs: 4997, Bias: 0.000000, T: 24878, Avg. loss: 0.094736\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 54.18, NNZs: 4997, Bias: 0.000000, T: 37317, Avg. loss: 0.069693\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 59.36, NNZs: 4997, Bias: 0.000000, T: 49756, Avg. loss: 0.054750\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 63.61, NNZs: 4997, Bias: 0.000000, T: 62195, Avg. loss: 0.044730\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 67.21, NNZs: 4997, Bias: 0.000000, T: 74634, Avg. loss: 0.037330\n",
            "Total training time: 1.76 seconds.\n",
            "Convergence after 6 epochs took 1.86 seconds\n",
            "-- Epoch 1\n",
            "Norm: 37.96, NNZs: 4995, Bias: 0.000000, T: 12440, Avg. loss: 0.193153\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 47.28, NNZs: 4995, Bias: 0.000000, T: 24880, Avg. loss: 0.092915\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 53.76, NNZs: 4995, Bias: 0.000000, T: 37320, Avg. loss: 0.067393\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 58.79, NNZs: 4995, Bias: 0.000000, T: 49760, Avg. loss: 0.052909\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 62.91, NNZs: 4995, Bias: 0.000000, T: 62200, Avg. loss: 0.043214\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 66.37, NNZs: 4995, Bias: 0.000000, T: 74640, Avg. loss: 0.036127\n",
            "Total training time: 1.74 seconds.\n",
            "Convergence after 6 epochs took 1.85 seconds\n",
            "-- Epoch 1\n",
            "Norm: 38.03, NNZs: 4994, Bias: 0.000000, T: 12440, Avg. loss: 0.198580\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 47.41, NNZs: 4994, Bias: 0.000000, T: 24880, Avg. loss: 0.097390\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 54.09, NNZs: 4994, Bias: 0.000000, T: 37320, Avg. loss: 0.072122\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 59.32, NNZs: 4994, Bias: 0.000000, T: 49760, Avg. loss: 0.057176\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 63.67, NNZs: 4994, Bias: 0.000000, T: 62200, Avg. loss: 0.047071\n",
            "Total training time: 1.44 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 67.32, NNZs: 4994, Bias: 0.000000, T: 74640, Avg. loss: 0.039612\n",
            "Total training time: 1.77 seconds.\n",
            "Convergence after 6 epochs took 1.87 seconds\n",
            "-- Epoch 1\n",
            "Norm: 38.28, NNZs: 4995, Bias: 0.000000, T: 12436, Avg. loss: 0.195637\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 47.43, NNZs: 4995, Bias: 0.000000, T: 24872, Avg. loss: 0.094741\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 54.00, NNZs: 4995, Bias: 0.000000, T: 37308, Avg. loss: 0.069871\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 59.14, NNZs: 4995, Bias: 0.000000, T: 49744, Avg. loss: 0.055407\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 63.33, NNZs: 4995, Bias: 0.000000, T: 62180, Avg. loss: 0.045525\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 66.89, NNZs: 4995, Bias: 0.000000, T: 74616, Avg. loss: 0.038435\n",
            "Total training time: 1.74 seconds.\n",
            "Convergence after 6 epochs took 1.83 seconds\n",
            "-- Epoch 1\n",
            "Norm: 37.88, NNZs: 4995, Bias: 0.000000, T: 12436, Avg. loss: 0.197071\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 47.06, NNZs: 4995, Bias: 0.000000, T: 24872, Avg. loss: 0.097303\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 53.68, NNZs: 4995, Bias: 0.000000, T: 37308, Avg. loss: 0.072305\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 59.02, NNZs: 4995, Bias: 0.000000, T: 49744, Avg. loss: 0.057736\n",
            "Total training time: 1.14 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 63.46, NNZs: 4995, Bias: 0.000000, T: 62180, Avg. loss: 0.047698\n",
            "Total training time: 1.45 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 67.17, NNZs: 4995, Bias: 0.000000, T: 74616, Avg. loss: 0.040157\n",
            "Total training time: 1.76 seconds.\n",
            "Convergence after 6 epochs took 1.85 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:38:27,495] Trial 4 finished with value: -0.944380639868745 and parameters: {'C': 0.4729781587434572, 'average': False, 'early_stopping': True, 'fit_intercept': False, 'max_iter': 298, 'tol': 0.05572514974917908, 'validation_fraction': 0.1456285383185577, 'n_iter_no_change=5': 5, 'shuffle': False, 'verbose': 1, 'n_jobs': 0, 'random_state': 5}. Best is trial 3 with value: -0.9561936013125513.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 21.98, NNZs: 4997, Bias: 0.000000, T: 7534, Avg. loss: 0.178440\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 28.14, NNZs: 4997, Bias: 0.000000, T: 15068, Avg. loss: 0.091582\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32.42, NNZs: 4997, Bias: 0.000000, T: 22602, Avg. loss: 0.069440\n",
            "Total training time: 0.75 seconds.\n",
            "Convergence after 3 epochs took 0.85 seconds\n",
            "-- Epoch 1\n",
            "Norm: 21.93, NNZs: 4999, Bias: 0.000000, T: 7536, Avg. loss: 0.179548\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 28.22, NNZs: 4999, Bias: 0.000000, T: 15072, Avg. loss: 0.092334\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32.42, NNZs: 4999, Bias: 0.000000, T: 22608, Avg. loss: 0.070143\n",
            "Total training time: 0.76 seconds.\n",
            "Convergence after 3 epochs took 0.87 seconds\n",
            "-- Epoch 1\n",
            "Norm: 21.95, NNZs: 4997, Bias: 0.000000, T: 7538, Avg. loss: 0.183230\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 28.21, NNZs: 4997, Bias: 0.000000, T: 15076, Avg. loss: 0.094954\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32.44, NNZs: 4997, Bias: 0.000000, T: 22614, Avg. loss: 0.072130\n",
            "Total training time: 0.76 seconds.\n",
            "Convergence after 3 epochs took 0.86 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22.03, NNZs: 4996, Bias: 0.000000, T: 7538, Avg. loss: 0.182802\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 28.35, NNZs: 4996, Bias: 0.000000, T: 15076, Avg. loss: 0.093892\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32.60, NNZs: 4996, Bias: 0.000000, T: 22614, Avg. loss: 0.070773\n",
            "Total training time: 0.75 seconds.\n",
            "Convergence after 3 epochs took 0.85 seconds\n",
            "-- Epoch 1\n",
            "Norm: 22.15, NNZs: 4998, Bias: 0.000000, T: 7543, Avg. loss: 0.181176\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 28.20, NNZs: 4998, Bias: 0.000000, T: 15086, Avg. loss: 0.093332\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 32.35, NNZs: 4998, Bias: 0.000000, T: 22629, Avg. loss: 0.071172\n",
            "Total training time: 0.76 seconds.\n",
            "Convergence after 3 epochs took 0.86 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:38:33,673] Trial 5 finished with value: -0.9307629204265793 and parameters: {'C': 0.16238623902036767, 'average': True, 'early_stopping': True, 'fit_intercept': False, 'max_iter': 965, 'tol': 0.03156467452077985, 'validation_fraction': 0.4819859388112281, 'n_iter_no_change=5': 2, 'shuffle': True, 'verbose': 1, 'n_jobs': 5, 'random_state': 0}. Best is trial 3 with value: -0.9561936013125513.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 43.06, NNZs: 4997, Bias: 0.000000, T: 9850, Avg. loss: 0.159366\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 52.61, NNZs: 4999, Bias: 0.000000, T: 19700, Avg. loss: 0.067407\n",
            "Total training time: 0.45 seconds.\n",
            "Convergence after 2 epochs took 0.55 seconds\n",
            "-- Epoch 1\n",
            "Norm: 42.61, NNZs: 4996, Bias: 0.000000, T: 9845, Avg. loss: 0.155253\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 51.81, NNZs: 4996, Bias: 0.000000, T: 19690, Avg. loss: 0.064891\n",
            "Total training time: 0.45 seconds.\n",
            "Convergence after 2 epochs took 0.56 seconds\n",
            "-- Epoch 1\n",
            "Norm: 42.93, NNZs: 4994, Bias: 0.000000, T: 9844, Avg. loss: 0.158411\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 52.46, NNZs: 4994, Bias: 0.000000, T: 19688, Avg. loss: 0.069003\n",
            "Total training time: 0.45 seconds.\n",
            "Convergence after 2 epochs took 0.55 seconds\n",
            "-- Epoch 1\n",
            "Norm: 42.55, NNZs: 4995, Bias: 0.000000, T: 9845, Avg. loss: 0.155399\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 51.96, NNZs: 4995, Bias: 0.000000, T: 19690, Avg. loss: 0.066903\n",
            "Total training time: 0.44 seconds.\n",
            "Convergence after 2 epochs took 0.54 seconds\n",
            "-- Epoch 1\n",
            "Norm: 41.99, NNZs: 4995, Bias: 0.000000, T: 9846, Avg. loss: 0.157120\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 51.48, NNZs: 4995, Bias: 0.000000, T: 19692, Avg. loss: 0.069548\n",
            "Total training time: 0.43 seconds.\n",
            "Convergence after 2 epochs took 0.54 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:38:38,306] Trial 6 finished with value: -0.9407711238720262 and parameters: {'C': 0.8936363643864574, 'average': False, 'early_stopping': True, 'fit_intercept': False, 'max_iter': 564, 'tol': 0.03665027817961133, 'validation_fraction': 0.32358485737839593, 'n_iter_no_change=5': 1, 'shuffle': False, 'verbose': 1, 'n_jobs': 5, 'random_state': 4}. Best is trial 3 with value: -0.9561936013125513.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 33.92, NNZs: 4994, Bias: 0.000000, T: 9975, Avg. loss: 0.172248\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.29, NNZs: 4995, Bias: 0.000000, T: 19950, Avg. loss: 0.083090\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.26, NNZs: 4995, Bias: 0.000000, T: 29925, Avg. loss: 0.060376\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.98, NNZs: 4995, Bias: 0.000000, T: 39900, Avg. loss: 0.046395\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.95, NNZs: 4995, Bias: 0.000000, T: 49875, Avg. loss: 0.037889\n",
            "Total training time: 1.52 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 60.23, NNZs: 4995, Bias: 0.000000, T: 59850, Avg. loss: 0.030874\n",
            "Total training time: 1.85 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 62.88, NNZs: 4995, Bias: 0.000000, T: 69825, Avg. loss: 0.025078\n",
            "Total training time: 2.17 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 65.23, NNZs: 4995, Bias: 0.000000, T: 79800, Avg. loss: 0.021576\n",
            "Total training time: 2.50 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 67.16, NNZs: 4995, Bias: 0.000000, T: 89775, Avg. loss: 0.018001\n",
            "Total training time: 2.82 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 68.91, NNZs: 4995, Bias: 0.000000, T: 99750, Avg. loss: 0.015856\n",
            "Total training time: 3.15 seconds.\n",
            "Convergence after 10 epochs took 3.25 seconds\n",
            "-- Epoch 1\n",
            "Norm: 33.80, NNZs: 4995, Bias: 0.000000, T: 9971, Avg. loss: 0.170914\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.24, NNZs: 4996, Bias: 0.000000, T: 19942, Avg. loss: 0.081504\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.15, NNZs: 4996, Bias: 0.000000, T: 29913, Avg. loss: 0.058013\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.83, NNZs: 4996, Bias: 0.000000, T: 39884, Avg. loss: 0.044445\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.60, NNZs: 4997, Bias: 0.000000, T: 49855, Avg. loss: 0.035627\n",
            "Total training time: 1.54 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59.61, NNZs: 4997, Bias: 0.000000, T: 59826, Avg. loss: 0.028825\n",
            "Total training time: 1.87 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 62.16, NNZs: 4997, Bias: 0.000000, T: 69797, Avg. loss: 0.024126\n",
            "Total training time: 2.19 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 64.40, NNZs: 4997, Bias: 0.000000, T: 79768, Avg. loss: 0.020604\n",
            "Total training time: 2.52 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 66.34, NNZs: 4997, Bias: 0.000000, T: 89739, Avg. loss: 0.017554\n",
            "Total training time: 2.85 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 68.03, NNZs: 4997, Bias: 0.000000, T: 99710, Avg. loss: 0.015217\n",
            "Total training time: 3.16 seconds.\n",
            "Convergence after 10 epochs took 3.27 seconds\n",
            "-- Epoch 1\n",
            "Norm: 33.68, NNZs: 4995, Bias: 0.000000, T: 9973, Avg. loss: 0.174266\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.28, NNZs: 4996, Bias: 0.000000, T: 19946, Avg. loss: 0.083790\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.36, NNZs: 4996, Bias: 0.000000, T: 29919, Avg. loss: 0.060268\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 53.05, NNZs: 4996, Bias: 0.000000, T: 39892, Avg. loss: 0.046657\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.88, NNZs: 4996, Bias: 0.000000, T: 49865, Avg. loss: 0.037549\n",
            "Total training time: 1.56 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 60.15, NNZs: 4996, Bias: 0.000000, T: 59838, Avg. loss: 0.030967\n",
            "Total training time: 1.90 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 62.83, NNZs: 4996, Bias: 0.000000, T: 69811, Avg. loss: 0.025690\n",
            "Total training time: 2.22 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 65.11, NNZs: 4996, Bias: 0.000000, T: 79784, Avg. loss: 0.021573\n",
            "Total training time: 2.55 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 67.10, NNZs: 4996, Bias: 0.000000, T: 89757, Avg. loss: 0.018711\n",
            "Total training time: 2.88 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 68.89, NNZs: 4996, Bias: 0.000000, T: 99730, Avg. loss: 0.016282\n",
            "Total training time: 3.20 seconds.\n",
            "Convergence after 10 epochs took 3.31 seconds\n",
            "-- Epoch 1\n",
            "Norm: 34.13, NNZs: 4997, Bias: 0.000000, T: 9966, Avg. loss: 0.171657\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42.54, NNZs: 4997, Bias: 0.000000, T: 19932, Avg. loss: 0.081971\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48.47, NNZs: 4997, Bias: 0.000000, T: 29898, Avg. loss: 0.058734\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 53.00, NNZs: 4997, Bias: 0.000000, T: 39864, Avg. loss: 0.045507\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.77, NNZs: 4997, Bias: 0.000000, T: 49830, Avg. loss: 0.036765\n",
            "Total training time: 1.56 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59.92, NNZs: 4997, Bias: 0.000000, T: 59796, Avg. loss: 0.030388\n",
            "Total training time: 1.89 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 62.64, NNZs: 4998, Bias: 0.000000, T: 69762, Avg. loss: 0.025651\n",
            "Total training time: 2.22 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 65.01, NNZs: 4998, Bias: 0.000000, T: 79728, Avg. loss: 0.021896\n",
            "Total training time: 2.54 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 66.92, NNZs: 4998, Bias: 0.000000, T: 89694, Avg. loss: 0.018260\n",
            "Total training time: 2.87 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 68.64, NNZs: 4998, Bias: 0.000000, T: 99660, Avg. loss: 0.016115\n",
            "Total training time: 3.20 seconds.\n",
            "Convergence after 10 epochs took 3.30 seconds\n",
            "-- Epoch 1\n",
            "Norm: 33.79, NNZs: 4996, Bias: 0.000000, T: 9967, Avg. loss: 0.173321\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 41.99, NNZs: 4996, Bias: 0.000000, T: 19934, Avg. loss: 0.084175\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47.83, NNZs: 4997, Bias: 0.000000, T: 29901, Avg. loss: 0.061098\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 52.51, NNZs: 4997, Bias: 0.000000, T: 39868, Avg. loss: 0.048595\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56.34, NNZs: 4997, Bias: 0.000000, T: 49835, Avg. loss: 0.039866\n",
            "Total training time: 1.55 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59.58, NNZs: 4997, Bias: 0.000000, T: 59802, Avg. loss: 0.033215\n",
            "Total training time: 1.89 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 62.44, NNZs: 4997, Bias: 0.000000, T: 69769, Avg. loss: 0.028385\n",
            "Total training time: 2.22 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 64.93, NNZs: 4998, Bias: 0.000000, T: 79736, Avg. loss: 0.024439\n",
            "Total training time: 2.54 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 67.08, NNZs: 4998, Bias: 0.000000, T: 89703, Avg. loss: 0.020896\n",
            "Total training time: 2.87 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 68.96, NNZs: 4998, Bias: 0.000000, T: 99670, Avg. loss: 0.018284\n",
            "Total training time: 3.20 seconds.\n",
            "Convergence after 10 epochs took 3.31 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:38:56,634] Trial 7 finished with value: -0.9420289855072463 and parameters: {'C': 0.40993845739947327, 'average': True, 'early_stopping': True, 'fit_intercept': False, 'max_iter': 1343, 'tol': 0.05308608750414457, 'validation_fraction': 0.31507143225121387, 'n_iter_no_change=5': 9, 'shuffle': True, 'verbose': 1, 'n_jobs': 0, 'random_state': 2}. Best is trial 3 with value: -0.9561936013125513.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 40.53, NNZs: 4994, Bias: 0.199304, T: 8087, Avg. loss: 0.150328\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 50.36, NNZs: 4994, Bias: 0.780306, T: 16174, Avg. loss: 0.067703\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 56.84, NNZs: 4994, Bias: 1.524827, T: 24261, Avg. loss: 0.044366\n",
            "Total training time: 0.76 seconds.\n",
            "Convergence after 3 epochs took 0.86 seconds\n",
            "-- Epoch 1\n",
            "Norm: 40.88, NNZs: 4993, Bias: -0.451517, T: 8084, Avg. loss: 0.149591\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 50.46, NNZs: 4995, Bias: 1.221945, T: 16168, Avg. loss: 0.065095\n",
            "Total training time: 0.47 seconds.\n",
            "Convergence after 2 epochs took 0.57 seconds\n",
            "-- Epoch 1\n",
            "Norm: 40.94, NNZs: 4995, Bias: -0.072782, T: 8084, Avg. loss: 0.153450\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 50.49, NNZs: 4997, Bias: 1.243139, T: 16168, Avg. loss: 0.068701\n",
            "Total training time: 0.47 seconds.\n",
            "Convergence after 2 epochs took 0.57 seconds\n",
            "-- Epoch 1\n",
            "Norm: 41.19, NNZs: 4994, Bias: 1.007350, T: 8081, Avg. loss: 0.152514\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 50.37, NNZs: 4995, Bias: 1.270847, T: 16162, Avg. loss: 0.063062\n",
            "Total training time: 0.46 seconds.\n",
            "Convergence after 2 epochs took 0.57 seconds\n",
            "-- Epoch 1\n",
            "Norm: 41.19, NNZs: 4994, Bias: 0.362330, T: 8088, Avg. loss: 0.154743\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 50.64, NNZs: 4994, Bias: 1.169399, T: 16176, Avg. loss: 0.065781\n",
            "Total training time: 0.47 seconds.\n",
            "Convergence after 2 epochs took 0.57 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:39:01,652] Trial 8 finished with value: -0.9440525020508612 and parameters: {'C': 0.6889174019050099, 'average': True, 'early_stopping': True, 'fit_intercept': True, 'max_iter': 1123, 'tol': 0.03532732911984717, 'validation_fraction': 0.44429224044892013, 'n_iter_no_change=5': 1, 'shuffle': True, 'verbose': 2, 'n_jobs': 3, 'random_state': 1}. Best is trial 3 with value: -0.9561936013125513.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 43.66, NNZs: 4989, Bias: 1.339585, T: 9081, Avg. loss: 0.162187\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.65, NNZs: 4990, Bias: 1.139849, T: 18162, Avg. loss: 0.069877\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 60.29, NNZs: 4990, Bias: 1.039784, T: 27243, Avg. loss: 0.045924\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 65.17, NNZs: 4990, Bias: 1.110141, T: 36324, Avg. loss: 0.032335\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 69.09, NNZs: 4990, Bias: 1.450740, T: 45405, Avg. loss: 0.024458\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 72.23, NNZs: 4990, Bias: 1.627713, T: 54486, Avg. loss: 0.019065\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 74.69, NNZs: 4990, Bias: 1.685099, T: 63567, Avg. loss: 0.014714\n",
            "Total training time: 1.66 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 76.69, NNZs: 4990, Bias: 1.507365, T: 72648, Avg. loss: 0.011623\n",
            "Total training time: 1.90 seconds.\n",
            "Convergence after 8 epochs took 2.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 43.74, NNZs: 4995, Bias: 1.191684, T: 9078, Avg. loss: 0.160476\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.62, NNZs: 4995, Bias: 1.198202, T: 18156, Avg. loss: 0.068684\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 60.34, NNZs: 4995, Bias: 0.843209, T: 27234, Avg. loss: 0.046211\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 65.44, NNZs: 4995, Bias: 0.670075, T: 36312, Avg. loss: 0.034149\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 69.40, NNZs: 4995, Bias: 0.965628, T: 45390, Avg. loss: 0.025331\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 72.48, NNZs: 4995, Bias: 1.009863, T: 54468, Avg. loss: 0.018822\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 74.89, NNZs: 4995, Bias: 1.394239, T: 63546, Avg. loss: 0.014398\n",
            "Total training time: 1.68 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 76.73, NNZs: 4995, Bias: 1.551992, T: 72624, Avg. loss: 0.010801\n",
            "Total training time: 1.93 seconds.\n",
            "Convergence after 8 epochs took 2.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 43.84, NNZs: 4994, Bias: 1.199017, T: 9079, Avg. loss: 0.164762\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.48, NNZs: 4994, Bias: 0.553156, T: 18158, Avg. loss: 0.071041\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 60.35, NNZs: 4995, Bias: 0.824439, T: 27237, Avg. loss: 0.048183\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 65.67, NNZs: 4995, Bias: 1.099758, T: 36316, Avg. loss: 0.035661\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 69.78, NNZs: 4995, Bias: 1.132692, T: 45395, Avg. loss: 0.026200\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 72.97, NNZs: 4995, Bias: 1.403903, T: 54474, Avg. loss: 0.019742\n",
            "Total training time: 1.43 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 75.42, NNZs: 4995, Bias: 1.525064, T: 63553, Avg. loss: 0.015071\n",
            "Total training time: 1.68 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 77.34, NNZs: 4995, Bias: 1.419129, T: 72632, Avg. loss: 0.011590\n",
            "Total training time: 1.92 seconds.\n",
            "Convergence after 8 epochs took 2.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 43.99, NNZs: 4996, Bias: 1.012137, T: 9077, Avg. loss: 0.161003\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.97, NNZs: 4996, Bias: 0.688672, T: 18154, Avg. loss: 0.068113\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 60.72, NNZs: 4997, Bias: 1.245376, T: 27231, Avg. loss: 0.044536\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 65.91, NNZs: 4997, Bias: 1.235664, T: 36308, Avg. loss: 0.032241\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 69.68, NNZs: 4997, Bias: 1.490065, T: 45385, Avg. loss: 0.023202\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 72.56, NNZs: 4997, Bias: 1.591631, T: 54462, Avg. loss: 0.017418\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 74.76, NNZs: 4997, Bias: 1.311421, T: 63539, Avg. loss: 0.013104\n",
            "Total training time: 1.66 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 76.49, NNZs: 4997, Bias: 1.324522, T: 72616, Avg. loss: 0.010328\n",
            "Total training time: 1.91 seconds.\n",
            "Convergence after 8 epochs took 2.01 seconds\n",
            "-- Epoch 1\n",
            "Norm: 43.79, NNZs: 4996, Bias: 2.666504, T: 9082, Avg. loss: 0.164481\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 53.64, NNZs: 4996, Bias: 2.479154, T: 18164, Avg. loss: 0.070836\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 60.55, NNZs: 4996, Bias: 2.426790, T: 27246, Avg. loss: 0.047700\n",
            "Total training time: 0.66 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 65.83, NNZs: 4996, Bias: 2.546777, T: 36328, Avg. loss: 0.034965\n",
            "Total training time: 0.92 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 69.84, NNZs: 4996, Bias: 2.479950, T: 45410, Avg. loss: 0.025750\n",
            "Total training time: 1.17 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 72.87, NNZs: 4996, Bias: 2.335426, T: 54492, Avg. loss: 0.019010\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 75.20, NNZs: 4996, Bias: 2.369406, T: 63574, Avg. loss: 0.014571\n",
            "Total training time: 1.67 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 77.04, NNZs: 4996, Bias: 2.457222, T: 72656, Avg. loss: 0.011366\n",
            "Total training time: 1.91 seconds.\n",
            "Convergence after 8 epochs took 2.02 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:39:13,653] Trial 9 finished with value: -0.934317746786984 and parameters: {'C': 0.739026144181476, 'average': False, 'early_stopping': True, 'fit_intercept': True, 'max_iter': 1362, 'tol': 0.02952602353242079, 'validation_fraction': 0.3759705010190727, 'n_iter_no_change=5': 7, 'shuffle': False, 'verbose': 2, 'n_jobs': 2, 'random_state': 1}. Best is trial 3 with value: -0.9561936013125513.\n",
            "[I 2020-10-01 17:39:20,560] Trial 10 finished with value: -0.9485917418649166 and parameters: {'C': 0.6659312028393137, 'average': False, 'early_stopping': False, 'fit_intercept': True, 'max_iter': 742, 'tol': 0.07668448406903002, 'validation_fraction': 0.10327651491260788, 'n_iter_no_change=5': 3, 'shuffle': True, 'verbose': 0, 'n_jobs': 1, 'random_state': 3}. Best is trial 3 with value: -0.9561936013125513.\n",
            "[I 2020-10-01 17:39:27,482] Trial 11 finished with value: -0.9485370522286025 and parameters: {'C': 0.6697044930630633, 'average': False, 'early_stopping': False, 'fit_intercept': True, 'max_iter': 680, 'tol': 0.07757703209031896, 'validation_fraction': 0.10141112359399829, 'n_iter_no_change=5': 3, 'shuffle': True, 'verbose': 0, 'n_jobs': 1, 'random_state': 3}. Best is trial 3 with value: -0.9561936013125513.\n",
            "[I 2020-10-01 17:39:48,576] Trial 12 finished with value: -0.95170905113481 and parameters: {'C': 0.9238005144317096, 'average': False, 'early_stopping': False, 'fit_intercept': True, 'max_iter': 847, 'tol': 0.0012456780909145954, 'validation_fraction': 0.665245605315496, 'n_iter_no_change=5': 3, 'shuffle': True, 'verbose': 0, 'n_jobs': 1, 'random_state': 3}. Best is trial 3 with value: -0.9561936013125513.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 57.57, NNZs: 4993, Bias: 1.451647, T: 14557, Avg. loss: 0.233360\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 70.74, NNZs: 4994, Bias: 1.298358, T: 29114, Avg. loss: 0.107834\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 79.92, NNZs: 4995, Bias: 1.646026, T: 43671, Avg. loss: 0.076616\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.93, NNZs: 4995, Bias: 1.353924, T: 58228, Avg. loss: 0.057976\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 92.45, NNZs: 4995, Bias: 1.607610, T: 72785, Avg. loss: 0.045732\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 96.98, NNZs: 4995, Bias: 0.399052, T: 87342, Avg. loss: 0.036948\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 100.41, NNZs: 4995, Bias: 1.390157, T: 101899, Avg. loss: 0.027446\n",
            "Total training time: 1.61 seconds.\n",
            "Convergence after 7 epochs took 1.61 seconds\n",
            "-- Epoch 1\n",
            "Norm: 57.55, NNZs: 4996, Bias: 1.362559, T: 14556, Avg. loss: 0.230310\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 70.36, NNZs: 4996, Bias: 1.321304, T: 29112, Avg. loss: 0.104944\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 79.49, NNZs: 4996, Bias: 1.342356, T: 43668, Avg. loss: 0.073934\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.17, NNZs: 4997, Bias: 1.494100, T: 58224, Avg. loss: 0.057104\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 91.54, NNZs: 4997, Bias: 1.398998, T: 72780, Avg. loss: 0.043970\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 95.81, NNZs: 4997, Bias: 0.485142, T: 87336, Avg. loss: 0.035025\n",
            "Total training time: 1.39 seconds.\n",
            "Convergence after 6 epochs took 1.39 seconds\n",
            "-- Epoch 1\n",
            "Norm: 56.94, NNZs: 4995, Bias: 1.038413, T: 14555, Avg. loss: 0.235963\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 70.46, NNZs: 4995, Bias: 0.492522, T: 29110, Avg. loss: 0.109471\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 79.87, NNZs: 4996, Bias: 1.371308, T: 43665, Avg. loss: 0.079180\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 87.01, NNZs: 4997, Bias: 1.640334, T: 58220, Avg. loss: 0.061473\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 92.80, NNZs: 4997, Bias: 1.583168, T: 72775, Avg. loss: 0.048948\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 97.40, NNZs: 4997, Bias: 0.662561, T: 87330, Avg. loss: 0.038481\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 101.09, NNZs: 4997, Bias: 1.581512, T: 101885, Avg. loss: 0.030880\n",
            "Total training time: 1.62 seconds.\n",
            "Convergence after 7 epochs took 1.62 seconds\n",
            "-- Epoch 1\n",
            "Norm: 56.72, NNZs: 4996, Bias: 0.646091, T: 14551, Avg. loss: 0.231030\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 69.59, NNZs: 4997, Bias: 0.567044, T: 29102, Avg. loss: 0.101693\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 78.64, NNZs: 4997, Bias: 1.556150, T: 43653, Avg. loss: 0.075268\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 85.78, NNZs: 4997, Bias: 1.437262, T: 58204, Avg. loss: 0.058439\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 91.17, NNZs: 4997, Bias: 1.355257, T: 72755, Avg. loss: 0.045101\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 95.63, NNZs: 4997, Bias: 1.318322, T: 87306, Avg. loss: 0.036547\n",
            "Total training time: 1.34 seconds.\n",
            "Convergence after 6 epochs took 1.35 seconds\n",
            "-- Epoch 1\n",
            "Norm: 56.55, NNZs: 4994, Bias: 0.896420, T: 14553, Avg. loss: 0.231548\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 69.91, NNZs: 4995, Bias: 0.571022, T: 29106, Avg. loss: 0.109928\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 79.21, NNZs: 4995, Bias: 1.033285, T: 43659, Avg. loss: 0.079586\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 86.15, NNZs: 4995, Bias: 0.975714, T: 58212, Avg. loss: 0.060040\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 91.92, NNZs: 4995, Bias: 1.447291, T: 72765, Avg. loss: 0.047743\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 96.38, NNZs: 4995, Bias: 1.691602, T: 87318, Avg. loss: 0.037802\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 100.31, NNZs: 4995, Bias: 0.595970, T: 101871, Avg. loss: 0.032676\n",
            "Total training time: 1.56 seconds.\n",
            "Convergence after 7 epochs took 1.56 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2020-10-01 17:39:57,542] Trial 13 finished with value: -0.9458572600492208 and parameters: {'C': 0.9834606712813241, 'average': False, 'early_stopping': False, 'fit_intercept': True, 'max_iter': 910, 'tol': 0.01683128932301605, 'validation_fraction': 0.6698336679351609, 'n_iter_no_change=5': 3, 'shuffle': True, 'verbose': 2, 'n_jobs': 1, 'random_state': 4}. Best is trial 3 with value: -0.9561936013125513.\n",
            "[I 2020-10-01 17:40:16,445] Trial 14 finished with value: -0.9513809133169264 and parameters: {'C': 0.9483787452878156, 'average': False, 'early_stopping': False, 'fit_intercept': True, 'max_iter': 1481, 'tol': 0.0023906872546575354, 'validation_fraction': 0.6249714119132936, 'n_iter_no_change=5': 4, 'shuffle': True, 'verbose': 0, 'n_jobs': 1, 'random_state': 4}. Best is trial 3 with value: -0.9561936013125513.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAg36g2Y6cFw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}